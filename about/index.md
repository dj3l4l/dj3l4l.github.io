---
layout: page
title: About
date: 2022-03-29
comments: false
---

## The Purpose of this Blog

This blog is a place for me to share fascinating insights on past and present breakthroughs in applied mathematics and probability theory. It is also a place for me to share the projects that I create; designed to illustrate the value (and I hope, the joy) of those breakthroughs and insights.

If a person comes away from this blog thinking, "wow, that *is* interesting, I never thought of it that way", then it will have achieved what I dreamed for it to do.

## A Long Tale of My Personal Interests and Motivations

At the start of 2022, I stopped working on a startup that I co-founded with a friend. At that point, I decided to spend the next year working solely on personal projects. 

These projects were designed to further my understanding of Bayesian decision theory. This may sound strange to some. Why spend a whole year on Bayesian decision theory? Is it really so interesting? Is it really so complex? And is the return on that sort of investment worth it? 

The answer I have to those questions is an emphatic "YES". The reason for that answer begins with the fact that, for much of my adult life, I have been absolutely fascinated (nay, obsessed) with explainable automated algorithms (or, in modern vernacular, Explainable AI). Explainable algorithms are those that are not only designed to make automated decisions, but that also provide human beings with *interpretable reasons* for those decisions. In the most fascinating of cases, explainable algorithms can teach human beings new things about the world around us.

And so, at the start of 2022, I asked myself: "How can I contribute to the future of explainable AI? What are the possible frontiers that my humble mind can comprehend?" 

After much deliberation, I came to believe that those frontiers would lie in undiscovered innovations of decision theory and probabilistic reasoning. After all, aren't the most valuable decisions those that are about uncertain outcomes? 

Then, if we are to create truly transparent "thinking machines", must we not find an objective way to translate any uncertainty into human language (and vice versa)?

Thankfully, such a daunting task is not one that we embark on from scratch. For the past hundred years or so, Bayesian probability theory has been used as an objective means to translate subjective knowledge into numerical form. It has even been proven that these objective rules mirror the axioms of formal logic (that is, if a decision violates the rules of Bayesian inference, then it must also violate the rules of logic).

Whilst reading the works of Bayesian pioneers (such as Jeffreys, Jaynes, Gibbs(!) and Laplace(!!)), it suddenly became apparent to me that a large body of work from the early 50s to the late 80s contained incredibly important, but mostly forgotten insights. 

Though perhaps not *completely* forgotten, many important discoveries regarding Bayesian inference remain trapped in academic niches. These niches were themselves stuck in a time when the computer was still in its infancy, and when analytical (not computational) solutions limited the scope of possible progress.

When I realised this, I decided to create a website (this website), which I would use to publish lesser-known, but important, concepts. My hopes are that the blogposts, projects and open-source code that I write will illustrate to others how valuable those consepts are. And perhaps even connect those ideas to more current and pressing issues. For example, the measurement and removal of unwanted bias from automated decision processes.

There is no way to know when a paradigm shift will occur in Explainable AI, or what it will look like when it does occur. But I am confident that, whatever and whenever such a breakthrough will happen, it will not be due to the discovery of new and exotic methods for computationally encoding and decoding **patterns**. 

Rather, it will come from **practical and applicable** answers to questions such as: "Which patterns in a given context constitute **knowledge**?", "What do we mean when we say we **learn**?" and "How can all forms of **subjective** knowledge be described by an **objective** and **universally interpretable** mathematical language?".


## A Brief Rundown of My Technical Achievements

In 2011, I received an MSci in Astronomy and Physics from UCL. My Masters thesis was on "Black Hole Thermodynamics and the Information Loss Paradox", and my advisor was [Prof. Ian Ford](https://www.ucl.ac.uk/physics-astronomy/people/professor-ian-ford).

I then went on to obtain a PhD in the Physics of Astronomical Detectors from the [Quantum Sensors Group](https://www.phy.cam.ac.uk/research/research-groups/qs) at the Cavendish Laboratory, University of Cambridge. My PhD advisor was [Prof. Stafford Withington](https://www.phy.cam.ac.uk/directory/withingtons), and my thesis was  titled "Thermal Transport and Noise in Micro-Engineered Support Structures for Detector Applications": an investigation in [novel methods for the accurate simulation of heat transport](https://aip.scitation.org/doi/10.1063/1.4893019), and for the utilisation of phase-coherent thermal waves to create low-dimensional "heat interferometers". The application of this form of heat interferometry was [proven empirically (for the first time)](https://aip.scitation.org/doi/10.1063/1.5041348) to reduce noise in superconducting detectors.

I graduated from Cambridge in 2016, and went on to co-found an augmented reality startup, called [Sention](https://www.linkedin.com/company/21145650/admin/), with my good friend [Alexander Bridi](https://www.linkedin.com/in/zanbridi/). Whilst at Sention, I researched and developed novel and innovative solutions to a broad range of unsolved computer vision problems: from 100% pixel-accurate real-time video segmentation, to 3D scene reconstruction from under-constrained single-camera views, to blind real-time lens distortion correction (to state just a few). All research and algorithms from my time at Sention were proprietary.

